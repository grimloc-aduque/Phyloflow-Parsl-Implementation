{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsl for workflow definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repository contains a Parsl implementation of the Phyloflow workflow. The code developed in Parsl is located inside the */parsl* directory. To follow the translation process see [documentation.ipynb](./documentation.ipynb). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow Context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phyloflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phyloflow is a phylogenetic tree calculation tool packaged with Docker and WDL. For more information on Phyloflow, see [github.com/ncsa/phyloflow](https://github.com/ncsa/phyloflow)\n",
    "\n",
    "<img src=\"./Images/phyloflow-workflow.png\" height=\"360\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <img src=\"./Images/wdl-logo.png\" height=\"50\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WDL (Workflow Description Language) is a way to specify data processing workflows. It is used extensively in scientific research focused mainly on Medical and Bioinformatics research. \n",
    "\n",
    "A common WDL script is comprised of a series of tasks that are called within a wokflow definition. The data flows within tasks through file dependencies. As an example here is the task for the pyclone-vi task:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"1\">\n",
    "\n",
    "```\n",
    "task pyclone_vi_clustering{\n",
    "    input {\n",
    "\t\tFile mutations_tsv\n",
    "    }\n",
    "\n",
    "\tcommand {\n",
    "\t\tsh /code/pyclone_vi_entrypoint.sh ${mutations_tsv}\n",
    "\t}\n",
    "\n",
    "\toutput {\n",
    "\t\tFile response = stdout()\n",
    "\t\tFile err_response = stderr()\n",
    "\t\tFile cluster_assignment = 'cluster_assignment.tsv'\n",
    "\t\t}\n",
    "\n",
    "\truntime {\n",
    "\t\tdocker: 'public.ecr.aws/k1t6h9x8/phyloflow/pyclone_vi:latest'\n",
    "\t\t}\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A task specifies the following sections:\n",
    "* Input: These are files commonly generated by other tasks\n",
    "* Command: A series of bash statements executed when input dependencies are met\n",
    "* Output: A definition of all the output files that could be used as dependencies by other tasks\n",
    "* Runtime: Typically a docker image with all the dependencies needed for the task to run\n",
    "\n",
    "\n",
    "The WDL code for the whole workflow is detailed in [/workflows/phyloflow_standalone.wdl](./workflows/phyloflow_standalone.wdl). This file specifies five tasks: vcf_transform, pyclone_vi, cluster_transform, spruce_phylogeny and aggregate_json, which are subsequently called inside the phyloflow workflow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <img src=\"./Images/parsl-logo.png\" height=\"50\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parsl extends parallelism in Python beyond a single computer*\n",
    "\n",
    "In the context of workflow definitions for scientific research, Parsl appears in an attempt to unify the flexibility of a complete programming language as Python, along with the ease of use of WDL for parallelizable workflow definitions.\n",
    "\n",
    "Parsl extends Python's syntax by implementing function decorators for @bash_app and @python_app. Both of these apps return AppFuture datatypes with the promise of execution once their dependencies are met. Apps receive and input array and generate an output array of DataFutures, which are commonly files. AppFutures are the building blocks of Parsl, just as tasks are from WDL. In Parsl, you compose a workflow by defining the output of an App as the input to another App. \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'./parsl/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from appfuture_manager import AppFutureManager\n",
    "from filesystem_util import (LOGS_DIR, ROOT, RUNS_DIR, format_files,\n",
    "                             get_stdfiles)\n",
    "from testing import *\n",
    "from workflow_tasks import *\n",
    "\n",
    "import parsl\n",
    "from parsl import bash_app, python_app\n",
    "from parsl.config import Config\n",
    "from parsl.dataflow.futures import AppFuture\n",
    "from parsl.executors import ThreadPoolExecutor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating WDL Tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every task of the WDL workflow was translated into 3 python functions: parsl app, get_inputs and run."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### App functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The app function is in charge of running the same code that was originally inside the command section of the WDL task description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bash_app\n",
    "def pyclone_vi(inputs=[], outputs=[], \n",
    "               stdout=None, stderr=None):\n",
    "    return f'''\n",
    "        conda run -n pyclone-vi pyclone-vi fit --in-file {inputs[0]} --out-file {outputs[0]}\n",
    "        conda run -n pyclone-vi pyclone-vi write-results-file --in-file {outputs[0]} --out-file {outputs[1]}\n",
    "        '''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get inputs functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_inputs function receives AppFutures previously called on the workflow that the app depends on, extracts the outputs needed (DataFutures) and groups them inside an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_pyclone_vi(vcf_future:AppFuture):\n",
    "    inputs = [\n",
    "        vcf_future.outputs[2]\n",
    "    ]\n",
    "    return inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run functions receive the inputs and the rundir where all outputs are going to be saved. These functions are in charge of defining the outputs and actually calling the app function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pyclone_vi(inputs:list, rundir:str) -> AppFuture:\n",
    "    outputs = [\n",
    "        'cluster_fit.hdf5',\n",
    "        'cluster_assignment.tsv'\n",
    "    ]\n",
    "    outputs = format_files(rundir, outputs)\n",
    "    stdout, stderr = get_stdfiles(rundir)\n",
    "    pyclone_future = pyclone_vi(inputs=inputs, outputs=outputs,\n",
    "                                stdout=stdout, stderr=stderr)\n",
    "    return pyclone_future"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python code for all the tasks is inside [/parsl/workflow_tasks.py](./parsl/workflow_tasks.py). \n",
    "\n",
    "The reasoning behind run functions relying solely on input files, and not AppFutures generated by other apps, is that this modular design allows testing the performance of a single app with test files, without the need to run all the previous steps in the workflow. Also, it allows to compose new workflows starting from any task. This was useful when integrating with the OpenAI function calling API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A series of adapter functions where created to execute the parsl apps from string inputs and appfuture ids. The adapter functions for all the tasks are defined inside [/parsl/function_calls.py](./parsl/function_calls.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcall_execute(run_function, inputs):\n",
    "    future_id = AppFutureManager.new_future_id(run_function)\n",
    "    future_dir = generate_subdir(AppFutureManager.DIR, future_id)\n",
    "    future = run_function(inputs, future_dir)\n",
    "    print(future)\n",
    "    AppFutureManager.index(future_id, future)\n",
    "    return future_id\n",
    "\n",
    "def fcall_from_files(run_function, inputs:list[str]):\n",
    "    inputs = format_files(ROOT, inputs)\n",
    "    return fcall_execute(run_function, inputs)\n",
    "\n",
    "def fcall_pyclone_vi_from_files(pyclone_vi_formatted:str):\n",
    "    inputs = [pyclone_vi_formatted]\n",
    "    return fcall_from_files(run_pyclone_vi, inputs)\n",
    "\n",
    "def fcall_pyclone_vi_from_futures(vcf_future_id:str):\n",
    "    vcf_future = AppFutureManager.query(vcf_future_id)\n",
    "    inputs = get_inputs_pyclone_vi(vcf_future)\n",
    "    return fcall_execute(run_pyclone_vi, inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unit tests of all the run functions are inside [/parsl/testing.py](./parsl/testing.py), where the test files are taken from the */example_data* directory. \n",
    "\n",
    "An example of a test function looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pyclone_vi():\n",
    "    fcall_pyclone_vi_from_files(\n",
    "        pyclone_vi_formatted=test_files['pyclone_vi_formatted']\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filesystem Managing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WDL automatically generates a folder structure for the workflow run, as well as a directory for every single task in the workflow. Meanwhile, in Parsl you have to explicitly create a folder structure in order to organize the outputs of your workflow. This translates into greater flexibility for the developer, at the price of needing a better degree of knowledge about the file system. The utility functions for creating the folder structure are defined inside [/parsl/filesystem_util.py](./parsl/filesystem_util.py)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of each run go into a subdirectory of the *runs* directory. The AppFutureManager class defined inside [/parsl/appfuture_manager.py](./parsl/appfuture_manager.py) is in charge of creating the subdirectories, generating unique identifiers for each executed future app, and map the future app identifier to its corresponding object reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_run_tree():\n",
    "    dir = os.path.join(RUNS_DIR, os.listdir(RUNS_DIR)[-1])\n",
    "    tree_output = ! tree {dir}\n",
    "    tree_output = '\\n'.join(list(tree_output))\n",
    "    special_chars = ['\u001b[01;34m', '\u001b[00m', '\u001b[0m', '\u001b[01;31m']\n",
    "    for chars in special_chars:\n",
    "        tree_output = tree_output.replace(chars, '')\n",
    "    print(tree_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alejo/Documents/NCSA/phyloflow_parsl/parsl/runs/example\n",
      "├── future_1_run_vcf_transform\n",
      "│   ├── headers.json\n",
      "│   ├── mutations.json\n",
      "│   ├── pyclone_samples\n",
      "│   │   └── A25.tsv\n",
      "│   ├── pyclone_vi_formatted.tsv\n",
      "│   ├── stderr.txt\n",
      "│   └── stdout.txt\n",
      "├── future_2_run_vcf_transform\n",
      "│   ├── headers.json\n",
      "│   ├── pyclone_samples\n",
      "│   ├── stderr.txt\n",
      "│   └── stdout.txt\n",
      "├── future_3_run_cluster_transform\n",
      "│   ├── spruce_formatted.tsv\n",
      "│   ├── stderr.txt\n",
      "│   └── stdout.txt\n",
      "├── future_4_run_spruce_tree\n",
      "│   ├── spruce.cliques\n",
      "│   ├── spruce.merged.res\n",
      "│   ├── spruce.res\n",
      "│   ├── spruce.res.gz\n",
      "│   ├── spruce.res.json\n",
      "│   ├── spruce.res.txt\n",
      "│   ├── stderr.txt\n",
      "│   └── stdout.txt\n",
      "└── future_5_run_aggregate_json\n",
      "    ├── aggregated.json\n",
      "    ├── stderr.txt\n",
      "    └── stdout.txt\n",
      "\n",
      "8 directories, 23 files\n"
     ]
    }
   ],
   "source": [
    "last_run_tree()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Workflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow function is only responsible for sending the outputs of one execution function as inputs to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcall_full_workflow(vep_vcf:str):    \n",
    "    vcf_future_id = fcall_vcf_transform_from_files(\n",
    "        vep_vcf=vep_vcf\n",
    "    )\n",
    "    pyclone_future_id = fcall_pyclone_vi_from_futures(\n",
    "        vcf_future_id=vcf_future_id\n",
    "    )\n",
    "    cluster_future_id = fcall_cluster_transform_from_futures(\n",
    "        vcf_future_id=vcf_future_id,\n",
    "        pyclone_future_id=pyclone_future_id\n",
    "    )\n",
    "    spruce_future_id = fcall_spruce_tree_from_futures(\n",
    "        cluster_future_id=cluster_future_id\n",
    "    )\n",
    "    aggregate_future_id = fcall_aggregate_json_from_futures(\n",
    "        vep_vcf=vep_vcf,\n",
    "        pyclone_future_id=pyclone_future_id,\n",
    "        spruce_future_id=spruce_future_id\n",
    "    )\n",
    "    return aggregate_future_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the test workflow function calls the workflow on an example data file and waits for the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_full_workflow():\n",
    "    future_id = fcall_full_workflow(\n",
    "        vep_vcf=test_files['vep_vcf']\n",
    "    )\n",
    "    AppFutureManager.query(future_id).result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsl requires to load a configuration that specifies the computing resources available, and how are they going to be distributed among parsl apps. In this case, a ThreadPoolExecutor is used with 4 as the maximum number of threads. The run_dir is also specified, which corresponds to the directory where all *log* information will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    config = Config(\n",
    "        executors=[\n",
    "            ThreadPoolExecutor(\n",
    "                label='threads',\n",
    "                max_threads=4\n",
    "            )\n",
    "        ],\n",
    "        run_dir=LOGS_DIR\n",
    "    )\n",
    "    parsl.load(config)\n",
    "\n",
    "load_config() # Can be called only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AppFuture at 0x7ff3cc581c90 state=pending>\n",
      "<AppFuture at 0x7ff3cc4d79d0 state=pending>\n",
      "<AppFuture at 0x7ff3cc504090 state=pending>\n",
      "<AppFuture at 0x7ff3cc504f10 state=pending>\n",
      "<AppFuture at 0x7ff3cc506c10 state=pending>\n"
     ]
    }
   ],
   "source": [
    "AppFutureManager.new_dir()\n",
    "test_full_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alejo/Documents/NCSA/phyloflow_parsl/parsl/runs/2023-07-25_13:29:55\n",
      "├── future_1_run_vcf_transform\n",
      "│   ├── headers.json\n",
      "│   ├── mutations.json\n",
      "│   ├── pyclone_samples\n",
      "│   │   └── A25.tsv\n",
      "│   ├── pyclone_vi_formatted.tsv\n",
      "│   ├── stderr.txt\n",
      "│   └── stdout.txt\n",
      "├── future_2_run_pyclone_vi\n",
      "│   ├── cluster_assignment.tsv\n",
      "│   ├── cluster_fit.hdf5\n",
      "│   ├── stderr.txt\n",
      "│   └── stdout.txt\n",
      "├── future_3_run_cluster_transform\n",
      "│   ├── spruce_formatted.tsv\n",
      "│   ├── stderr.txt\n",
      "│   └── stdout.txt\n",
      "├── future_4_run_spruce_tree\n",
      "│   ├── spruce.cliques\n",
      "│   ├── spruce.merged.res\n",
      "│   ├── spruce.res\n",
      "│   ├── spruce.res.gz\n",
      "│   ├── spruce.res.json\n",
      "│   ├── spruce.res.txt\n",
      "│   ├── stderr.txt\n",
      "│   └── stdout.txt\n",
      "└── future_5_run_aggregate_json\n",
      "    ├── aggregated.json\n",
      "    ├── stderr.txt\n",
      "    └── stdout.txt\n",
      "\n",
      "7 directories, 24 files\n"
     ]
    }
   ],
   "source": [
    "last_run_tree()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the functionality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original workflow functionality was extended to process multiple input files.The output files of all workflows are concatenated using one last @python_app called aggregate_workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@python_app\n",
    "def aggregate_workflows(inputs=[], outputs=[]):\n",
    "    output_json = []\n",
    "    for file in inputs:\n",
    "        workflow_json = json.load(open(file))\n",
    "        output_json.append(workflow_json)\n",
    "    output_file = open(outputs[0], 'w')\n",
    "    output_file.write(json.dumps(output_json))\n",
    "    output_file.close()\n",
    "\n",
    "def get_inputs_aggregate_workflows(aggregate_futures:List[AppFuture]):\n",
    "    return [future.outputs[0] for future in aggregate_futures]\n",
    "\n",
    "def run_aggregate_workflows(inputs:list, rundir):\n",
    "    outputs = [\n",
    "        'aggregated_workflows.json'\n",
    "    ]\n",
    "    outputs = format_files(rundir, outputs)\n",
    "    aggregate_workflows_future = aggregate_workflows(inputs=inputs, outputs=outputs)\n",
    "    return aggregate_workflows_future"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final parallel workflow implementation is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcall_parallel_workflows(vep_vcf_files:list[str]):\n",
    "    future_ids = []\n",
    "    for vep_vcf in vep_vcf_files:\n",
    "        future_id = fcall_full_workflow(\n",
    "            vep_vcf=vep_vcf\n",
    "        )\n",
    "        future_ids.append(future_id)\n",
    "\n",
    "    futures = [AppFutureManager.query(id) for id in future_ids]\n",
    "    inputs = get_inputs_aggregate_workflows(futures)\n",
    "    return fcall_execute(run_aggregate_workflows, inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, the test function launches 3 workflows over the same input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parallel_workflows():\n",
    "    future_id = fcall_parallel_workflows(\n",
    "        vep_vcf_files=[test_files['vep_vcf']]*3\n",
    "    )\n",
    "    AppFutureManager.query(future_id).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AppFuture at 0x7ff3cc507f10 state=pending>\n",
      "<AppFuture at 0x7ff3cc51e410 state=pending>\n",
      "<AppFuture at 0x7ff3cc51c7d0 state=pending>\n",
      "<AppFuture at 0x7ff3cc51d690 state=pending>\n",
      "<AppFuture at 0x7ff3cc51edd0 state=pending>\n",
      "<AppFuture at 0x7ff3cc3300d0 state=pending>\n",
      "<AppFuture at 0x7ff3cc330910 state=pending>\n",
      "<AppFuture at 0x7ff3cc331410 state=pending>\n",
      "<AppFuture at 0x7ff3cc332310 state=pending>\n",
      "<AppFuture at 0x7ff3cc333690 state=pending>\n",
      "<AppFuture at 0x7ff3cc340e50 state=pending>\n",
      "<AppFuture at 0x7ff3cc341f90 state=pending>\n",
      "<AppFuture at 0x7ff3cc342e50 state=pending>\n",
      "<AppFuture at 0x7ff3cc343690 state=pending>\n",
      "<AppFuture at 0x7ff3cc34d390 state=pending>\n",
      "<AppFuture at 0x7ff3cc34e210 state=pending>\n"
     ]
    }
   ],
   "source": [
    "AppFutureManager.new_dir()\n",
    "test_parallel_workflows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker Container"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/docker.png\" height=\"100\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WDL is designed to run every single task in an independent container, so the original phyloflow had different docker images for every task. That is not the case for Parsl, however a workaround was to create multiple conda environments within the same docker container, so that parsl apps can execute the code within the corresponding environment.\n",
    "\n",
    "Attached to the project is the dockerfile ([/parsl/docker/dockerfile](./parsl/docker/dockerfile)) that contains the definition to build a docker image with all the dependencies that the workflow needs, including the conda environments. The file [/parsl/docker/docker_commands.sh](./parsl/docker/docker_commands.sh) contains docker commands to build the image and run the container. \n",
    "\n",
    "The container has been tested on Linux and Mac Systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook documentation.ipynb to markdown\n",
      "[NbConvertApp] Writing 15772 bytes to README.md\n"
     ]
    }
   ],
   "source": [
    "# Convert notebook to readme\n",
    "! jupyter nbconvert documentation.ipynb --to markdown --output README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phyloflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
