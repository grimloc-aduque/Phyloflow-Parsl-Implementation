{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsl for workflow definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repository contains a Parsl implementation of the Phyloflow workflow. The code developed in Parsl is located inside the */parsl* directory. To follow the translation process see [documentation.ipynb](./documentation.ipynb). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow Context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phyloflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phyloflow is a phylogenetic tree calculation tool packaged with Docker and WDL. For more information on Phyloflow, see [github.com/ncsa/phyloflow](https://github.com/ncsa/phyloflow)\n",
    "\n",
    "<img src=\"./Images/phyloflow-workflow.png\" height=\"360\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <img src=\"./Images/wdl-logo.png\" height=\"50\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WDL (Workflow Description Language) is a way to specify data processing workflows. It is used extensively in scientific research focused mainly on Medical and Bioinformatics research. \n",
    "\n",
    "A common WDL script is comprised of a series of tasks that are called within a wokflow definition. The data flows within tasks through file dependencies. As an example here is the task for the pyclone-vi task:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"1\">\n",
    "\n",
    "```\n",
    "task pyclone_vi_clustering{\n",
    "    input {\n",
    "\t\tFile mutations_tsv\n",
    "    }\n",
    "\n",
    "\tcommand {\n",
    "\t\tsh /code/pyclone_vi_entrypoint.sh ${mutations_tsv}\n",
    "\t}\n",
    "\n",
    "\toutput {\n",
    "\t\tFile response = stdout()\n",
    "\t\tFile err_response = stderr()\n",
    "\t\tFile cluster_assignment = 'cluster_assignment.tsv'\n",
    "\t\t}\n",
    "\n",
    "\truntime {\n",
    "\t\tdocker: 'public.ecr.aws/k1t6h9x8/phyloflow/pyclone_vi:latest'\n",
    "\t\t}\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A task specifies the following sections:\n",
    "* Input: These are files commonly generated by other tasks\n",
    "* Command: A series of bash statements executed when input dependencies are met\n",
    "* Output: A definition of all the output files that could be used as dependencies by other tasks\n",
    "* Runtime: Typically a docker image with all the dependencies needed for the task to run\n",
    "\n",
    "\n",
    "The WDL code for the whole workflow is detailed in [/workflows/phyloflow_standalone.wdl](./workflows/phyloflow_standalone.wdl). This file specifies five tasks: vcf_transform, pyclone_vi, cluster_transform, spruce_phylogeny and aggregate_json, which are subsequently called inside the phyloflow workflow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <img src=\"./Images/parsl-logo.png\" height=\"50\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parsl extends parallelism in Python beyond a single computer*\n",
    "\n",
    "In the context of workflow definitions for scientific research, Parsl appears in an attempt to unify the flexibility of a complete programming language as Python, along with the ease of use of WDL for parallelizable workflow definitions.\n",
    "\n",
    "Parsl extends Python's syntax by implementing function decorators for @bash_app and @python_app. Both of these apps return AppFuture datatypes with the promise of execution once their dependencies are met. Apps receive and input array and generate an output array of DataFutures, which are commonly files. AppFutures are the building blocks of Parsl, just as tasks are from WDL. In Parsl, you compose a workflow by defining the output of an App as the input to another App. \n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'./parsl/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from time import sleep, time\n",
    "from typing import List\n",
    "\n",
    "from filesystem_util import (DATA_DIR, LOGS_DIR, ROOT, RUNS_DIR, TEST_DIR,\n",
    "                             format_files, generate_run_dir, generate_task_dir,\n",
    "                             generate_workflow_dir, get_stdfiles)\n",
    "from tasks import *\n",
    "from testing import *\n",
    "\n",
    "import parsl\n",
    "from parsl import bash_app, python_app\n",
    "from parsl.config import Config\n",
    "from parsl.data_provider.files import File\n",
    "from parsl.dataflow.futures import AppFuture\n",
    "from parsl.executors import ThreadPoolExecutor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating WDL Tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every task of the WDL workflow was translated into 3 python functions: parsl app, get_inputs and run."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### App functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The app function is in charge of running the same code that was originally inside the command section of the WDL task description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bash_app\n",
    "def pyclone_vi(inputs=[], outputs=[], \n",
    "               stdout=None, stderr=None):\n",
    "    return f'''\n",
    "        conda run -n pyclone-vi pyclone-vi fit --in-file {inputs[0]} --out-file {outputs[0]}\n",
    "        conda run -n pyclone-vi pyclone-vi write-results-file --in-file {outputs[0]} --out-file {outputs[1]}\n",
    "        '''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get inputs functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_inputs function receives AppFutures previously called on the workflow that the app depends on, extracts the outputs needed (DataFutures) and groups them inside an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_pyclone_vi(vcf_future:AppFuture):\n",
    "    inputs = [\n",
    "        vcf_future.outputs[2]\n",
    "    ]\n",
    "    return inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run functions receive the inputs and the rundir where all outputs are going to be saved. These functions are in charge of defining the outputs and actually calling the app function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pyclone_vi(inputs:list, rundir:str) -> AppFuture:\n",
    "    outdir = generate_task_dir(rundir)\n",
    "    outputs = [\n",
    "        'cluster_fit.hdf5',\n",
    "        'cluster_assignment.tsv'\n",
    "    ]\n",
    "    outputs = format_files(outdir, outputs)\n",
    "    stdout, stderr = get_stdfiles(outdir)\n",
    "    pyclone_future = pyclone_vi(inputs=inputs, outputs=outputs,\n",
    "                                stdout=stdout, stderr=stderr)\n",
    "    return pyclone_future"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python code for all the tasks is inside [/parsl/tasks.py](./parsl/tasks.py). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reasoning behind run functions relying solely on input files, and not AppFutures generated by other apps, is that this modular design allows testing the performance of a single app with test files, without the need to run all the previous steps in the workflow. The unit tests of all the run functions are inside [/parsl/testing.py](./parsl/testing.py), where the test files are taken from the */example_data* directory. \n",
    "\n",
    "An example of a test function looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_task(inputs, run_task_func):\n",
    "    inputs = format_files(DATA_DIR, inputs)\n",
    "    future = run_task_func(inputs, TEST_DIR)\n",
    "    print(future)\n",
    "    future.result()\n",
    "\n",
    "def test_pyclone_vi():\n",
    "    inputs = ['pyclone_vi_formatted.tsv']\n",
    "    test_task(inputs, run_pyclone_vi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filesystem Managing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WDL automatically generates a folder structure for the workflow run, as well as a directory for every single task in the workflow. Meanwhile, in Parsl you have to explicitly create a folder structure in order to organize the outputs of your workflow. This translates into greater flexibility for the developer, at the price of needing a better degree of knowledge about the file system. The utility functions for creating the folder structure are defined inside [/parsl/filesystem_util.py](./parsl/filesystem_util.py)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs of the tasks tests go to the */parsl/tests* directory, that contains subdirectories for every test. These subdirectories are overwritten everytime the same test is run. The whole directory tree has the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./parsl/tests\u001b[0m\n",
      "├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "│   ├── \u001b[00maggregated.json\u001b[0m\n",
      "│   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   └── \u001b[00mstdout.txt\u001b[0m\n",
      "├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "│   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "│   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   └── \u001b[00mstdout.txt\u001b[0m\n",
      "├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "│   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "│   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "│   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   └── \u001b[00mstdout.txt\u001b[0m\n",
      "├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "│   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "│   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "│   ├── \u001b[00mspruce.res\u001b[0m\n",
      "│   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "│   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "│   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "│   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   └── \u001b[00mstdout.txt\u001b[0m\n",
      "└── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "    ├── \u001b[00mheaders.json\u001b[0m\n",
      "    ├── \u001b[00mmutations.json\u001b[0m\n",
      "    ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "    │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "    ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "    ├── \u001b[00mstderr.txt\u001b[0m\n",
      "    └── \u001b[00mstdout.txt\u001b[0m\n",
      "\n",
      "7 directories, 24 files\n"
     ]
    }
   ],
   "source": [
    "! tree ./parsl/tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs of a complete workflow run goes to the */pars/runs* directory. For every run a new subdirectory is created. The folder structure for two workflow runs looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./parsl/runs\u001b[0m\n",
      "├── \u001b[01;34m2023-06-22_09:53:18\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "│   │   ├── \u001b[00maggregated.json\u001b[0m\n",
      "│   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "│   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "│   │   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "│   │   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "│   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce.res\u001b[0m\n",
      "│   │   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "│   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "│   │   ├── \u001b[00mheaders.json\u001b[0m\n",
      "│   │   ├── \u001b[00mmutations.json\u001b[0m\n",
      "│   │   ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "│   │   │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "│   │   ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "│   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   └── \u001b[00mVEP_raw.A25.mutect2.filtered.snp.vcf\u001b[0m\n",
      "├── \u001b[01;34m2023-06-22_09:53:24\u001b[0m\n",
      "│   ├── \u001b[00maggregated_workflows.json\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_workflow_0\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "│   │   │   ├── \u001b[00maggregated.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "│   │   │   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res\u001b[0m\n",
      "│   │   │   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "│   │   │   ├── \u001b[00mheaders.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mmutations.json\u001b[0m\n",
      "│   │   │   ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "│   │   │   │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   └── \u001b[00mVEP_raw.A25.mutect2.filtered.snp.vcf\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_workflow_1\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "│   │   │   ├── \u001b[00maggregated.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "│   │   │   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res\u001b[0m\n",
      "│   │   │   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "│   │   │   ├── \u001b[00mheaders.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mmutations.json\u001b[0m\n",
      "│   │   │   ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "│   │   │   │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   └── \u001b[00mVEP_raw.A25.mutect2.filtered.snp.vcf\u001b[0m\n",
      "│   └── \u001b[01;34mrun_workflow_2\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "│       │   ├── \u001b[00maggregated.json\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "│       │   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "│       │   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.res\u001b[0m\n",
      "│       │   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "│       │   ├── \u001b[00mheaders.json\u001b[0m\n",
      "│       │   ├── \u001b[00mmutations.json\u001b[0m\n",
      "│       │   ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "│       │   │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "│       │   ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       └── \u001b[00mVEP_raw.A25.mutect2.filtered.snp.vcf\u001b[0m\n",
      "├── \u001b[01;34m2023-06-22_10:02:45\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "│   │   ├── \u001b[00maggregated.json\u001b[0m\n",
      "│   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "│   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "│   │   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "│   │   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "│   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce.res\u001b[0m\n",
      "│   │   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "│   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "│   │   ├── \u001b[00mheaders.json\u001b[0m\n",
      "│   │   ├── \u001b[00mmutations.json\u001b[0m\n",
      "│   │   ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "│   │   │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "│   │   ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "│   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   └── \u001b[00mVEP_raw.A25.mutect2.filtered.snp.vcf\u001b[0m\n",
      "├── \u001b[01;34m2023-06-22_10:02:50\u001b[0m\n",
      "│   ├── \u001b[00maggregated_workflows.json\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_workflow_0\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "│   │   │   ├── \u001b[00maggregated.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "│   │   │   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res\u001b[0m\n",
      "│   │   │   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "│   │   │   ├── \u001b[00mheaders.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mmutations.json\u001b[0m\n",
      "│   │   │   ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "│   │   │   │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   └── \u001b[00mVEP_raw.A25.mutect2.filtered.snp.vcf\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_workflow_1\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "│   │   │   ├── \u001b[00maggregated.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "│   │   │   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res\u001b[0m\n",
      "│   │   │   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "│   │   │   ├── \u001b[00mheaders.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mmutations.json\u001b[0m\n",
      "│   │   │   ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "│   │   │   │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   └── \u001b[00mVEP_raw.A25.mutect2.filtered.snp.vcf\u001b[0m\n",
      "│   └── \u001b[01;34mrun_workflow_2\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "│       │   ├── \u001b[00maggregated.json\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "│       │   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "│       │   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.res\u001b[0m\n",
      "│       │   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "│       │   ├── \u001b[00mheaders.json\u001b[0m\n",
      "│       │   ├── \u001b[00mmutations.json\u001b[0m\n",
      "│       │   ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "│       │   │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "│       │   ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       └── \u001b[00mVEP_raw.A25.mutect2.filtered.snp.vcf\u001b[0m\n",
      "├── \u001b[01;34m2023-06-22_10:47:35\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "│   │   ├── \u001b[00maggregated.json\u001b[0m\n",
      "│   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "│   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "│   │   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "│   │   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "│   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce.res\u001b[0m\n",
      "│   │   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "│   │   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "│   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "│   │   ├── \u001b[00mheaders.json\u001b[0m\n",
      "│   │   ├── \u001b[00mmutations.json\u001b[0m\n",
      "│   │   ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "│   │   │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "│   │   ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "│   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   └── \u001b[00mVEP_raw.A25.mutect2.filtered.snp.vcf\u001b[0m\n",
      "├── \u001b[01;34m2023-06-22_10:47:56\u001b[0m\n",
      "│   ├── \u001b[00maggregated_workflows.json\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_workflow_0\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "│   │   │   ├── \u001b[00maggregated.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "│   │   │   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res\u001b[0m\n",
      "│   │   │   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "│   │   │   ├── \u001b[00mheaders.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mmutations.json\u001b[0m\n",
      "│   │   │   ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "│   │   │   │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   └── \u001b[00mVEP_raw.A25.mutect2.filtered.snp.vcf\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_workflow_1\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "│   │   │   ├── \u001b[00maggregated.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "│   │   │   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res\u001b[0m\n",
      "│   │   │   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "│   │   │   ├── \u001b[00mheaders.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mmutations.json\u001b[0m\n",
      "│   │   │   ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "│   │   │   │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   └── \u001b[00mVEP_raw.A25.mutect2.filtered.snp.vcf\u001b[0m\n",
      "│   └── \u001b[01;34mrun_workflow_2\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "│       │   ├── \u001b[00maggregated.json\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "│       │   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "│       │   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.res\u001b[0m\n",
      "│       │   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "│       │   ├── \u001b[00mheaders.json\u001b[0m\n",
      "│       │   ├── \u001b[00mmutations.json\u001b[0m\n",
      "│       │   ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "│       │   │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "│       │   ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       └── \u001b[00mVEP_raw.A25.mutect2.filtered.snp.vcf\u001b[0m\n",
      "├── \u001b[01;34mexample_parallel_run\u001b[0m\n",
      "│   ├── \u001b[00maggregated_workflows.json\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_workflow_0\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "│   │   │   ├── \u001b[00maggregated.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "│   │   │   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res\u001b[0m\n",
      "│   │   │   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "│   │   │   ├── \u001b[00mheaders.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mmutations.json\u001b[0m\n",
      "│   │   │   ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "│   │   │   │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   └── \u001b[00mVEP_raw.A25.mutect2.filtered.snp.vcf\u001b[0m\n",
      "│   ├── \u001b[01;34mrun_workflow_1\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "│   │   │   ├── \u001b[00maggregated.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "│   │   │   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res\u001b[0m\n",
      "│   │   │   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   ├── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "│   │   │   ├── \u001b[00mheaders.json\u001b[0m\n",
      "│   │   │   ├── \u001b[00mmutations.json\u001b[0m\n",
      "│   │   │   ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "│   │   │   │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "│   │   │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   │   │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│   │   └── \u001b[00mVEP_raw.A25.mutect2.filtered.snp.vcf\u001b[0m\n",
      "│   └── \u001b[01;34mrun_workflow_2\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "│       │   ├── \u001b[00maggregated.json\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "│       │   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "│       │   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.res\u001b[0m\n",
      "│       │   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "│       │   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       ├── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "│       │   ├── \u001b[00mheaders.json\u001b[0m\n",
      "│       │   ├── \u001b[00mmutations.json\u001b[0m\n",
      "│       │   ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "│       │   │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "│       │   ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "│       │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│       │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "│       └── \u001b[00mVEP_raw.A25.mutect2.filtered.snp.vcf\u001b[0m\n",
      "└── \u001b[01;34mexample_workflow_run\u001b[0m\n",
      "    ├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "    │   ├── \u001b[00maggregated.json\u001b[0m\n",
      "    │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "    │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "    ├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "    │   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "    │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "    │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "    ├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "    │   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "    │   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "    │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "    │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "    ├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "    │   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "    │   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "    │   ├── \u001b[00mspruce.res\u001b[0m\n",
      "    │   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "    │   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "    │   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "    │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "    │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "    ├── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "    │   ├── \u001b[00mheaders.json\u001b[0m\n",
      "    │   ├── \u001b[00mmutations.json\u001b[0m\n",
      "    │   ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "    │   │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "    │   ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "    │   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "    │   └── \u001b[00mstdout.txt\u001b[0m\n",
      "    └── \u001b[00mVEP_raw.A25.mutect2.filtered.snp.vcf\u001b[0m\n",
      "\n",
      "117 directories, 404 files\n"
     ]
    }
   ],
   "source": [
    "! tree ./parsl/runs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Workflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow function is only responsible for sending the outputs of one execution function as inputs to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_workflow(input_file:File, rundir:str) -> AppFuture:\n",
    "    print(f\"\\nScheduling Workflow\")\n",
    "    print(f\"Input file: {input_file}\")\n",
    "    print(f\"Output dir: {rundir}\")\n",
    "\n",
    "    shutil.copy(input_file, rundir)\n",
    "    inputs = [input_file]\n",
    "    vcf_future = run_vcf_transform(inputs, rundir)\n",
    "\n",
    "    inputs = get_inputs_pyclone_vi(vcf_future)\n",
    "    pyclone_future = run_pyclone_vi(inputs, rundir)\n",
    "\n",
    "    inputs = get_inputs_cluster_transform(vcf_future, pyclone_future)\n",
    "    cluster_future = run_cluster_transform(inputs, rundir)\n",
    "\n",
    "    inputs = get_inputs_spruce_tree(cluster_future)\n",
    "    spruce_future = run_spruce_tree(inputs, rundir)\n",
    "\n",
    "    inputs = get_inputs_aggregate_json(input_file, pyclone_future, spruce_future)\n",
    "    aggregate_json_future = run_aggregate_json(inputs, rundir)\n",
    "    \n",
    "    futures = [vcf_future, pyclone_future, cluster_future, \n",
    "               spruce_future, aggregate_json_future]\n",
    "    [print(f) for f in futures]\n",
    "    return aggregate_json_future"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the test workflow function calls the workflow on an example data file and waits for the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_workflow():\n",
    "    rundir = generate_run_dir(RUNS_DIR)\n",
    "    inputs = [File('VEP_raw.A25.mutect2.filtered.snp.vcf')]\n",
    "    inputs = format_files(DATA_DIR, inputs)\n",
    "    future = run_workflow(inputs[0], rundir)\n",
    "    print(future)\n",
    "    future.result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsl requires to load a configuration that specifies the computing resources available, and how are they going to be distributed among parsl apps. In this case, a ThreadPoolExecutor is used with 4 as the maximum number of threads. The run_dir is also specified, which corresponds to the directory where all *log* information will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    config = Config(\n",
    "        executors=[\n",
    "            ThreadPoolExecutor(\n",
    "                label='threads',\n",
    "                max_threads=4\n",
    "            )\n",
    "        ],\n",
    "        run_dir=LOGS_DIR\n",
    "    )\n",
    "    parsl.load(config)\n",
    "\n",
    "load_config() # Can be called only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scheduling Workflow\n",
      "Input file: /home/alejo/Documents/NCSA/phyloflow_parsl/example_data/VEP_raw.A25.mutect2.filtered.snp.vcf\n",
      "Output dir: /home/alejo/Documents/NCSA/phyloflow_parsl/parsl/runs/2023-06-22_10:47:35\n",
      "<AppFuture at 0x7f77fc265ad0 state=pending>\n",
      "<AppFuture at 0x7f77e6a63ad0 state=pending>\n",
      "<AppFuture at 0x7f77e6a8c190 state=pending>\n",
      "<AppFuture at 0x7f77e6a8d2d0 state=pending>\n",
      "<AppFuture at 0x7f77e6a8eb90 state=pending>\n",
      "<AppFuture at 0x7f77e6a8eb90 state=pending>\n"
     ]
    }
   ],
   "source": [
    "test_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/alejo/Documents/NCSA/phyloflow_parsl/parsl/runs/2023-06-22_10:47:35\u001b[0m\n",
      "├── \u001b[01;34mrun_aggregate_json\u001b[0m\n",
      "│   ├── \u001b[00maggregated.json\u001b[0m\n",
      "│   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   └── \u001b[00mstdout.txt\u001b[0m\n",
      "├── \u001b[01;34mrun_cluster_transform\u001b[0m\n",
      "│   ├── \u001b[00mspruce_formatted.tsv\u001b[0m\n",
      "│   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   └── \u001b[00mstdout.txt\u001b[0m\n",
      "├── \u001b[01;34mrun_pyclone_vi\u001b[0m\n",
      "│   ├── \u001b[00mcluster_assignment.tsv\u001b[0m\n",
      "│   ├── \u001b[00mcluster_fit.hdf5\u001b[0m\n",
      "│   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   └── \u001b[00mstdout.txt\u001b[0m\n",
      "├── \u001b[01;34mrun_spruce_tree\u001b[0m\n",
      "│   ├── \u001b[00mspruce.cliques\u001b[0m\n",
      "│   ├── \u001b[00mspruce.merged.res\u001b[0m\n",
      "│   ├── \u001b[00mspruce.res\u001b[0m\n",
      "│   ├── \u001b[01;31mspruce.res.gz\u001b[0m\n",
      "│   ├── \u001b[00mspruce.res.json\u001b[0m\n",
      "│   ├── \u001b[00mspruce.res.txt\u001b[0m\n",
      "│   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   └── \u001b[00mstdout.txt\u001b[0m\n",
      "├── \u001b[01;34mrun_vcf_transform\u001b[0m\n",
      "│   ├── \u001b[00mheaders.json\u001b[0m\n",
      "│   ├── \u001b[00mmutations.json\u001b[0m\n",
      "│   ├── \u001b[01;34mpyclone_samples\u001b[0m\n",
      "│   │   └── \u001b[00mA25.tsv\u001b[0m\n",
      "│   ├── \u001b[00mpyclone_vi_formatted.tsv\u001b[0m\n",
      "│   ├── \u001b[00mstderr.txt\u001b[0m\n",
      "│   └── \u001b[00mstdout.txt\u001b[0m\n",
      "└── \u001b[00mVEP_raw.A25.mutect2.filtered.snp.vcf\u001b[0m\n",
      "\n",
      "7 directories, 25 files\n"
     ]
    }
   ],
   "source": [
    "! tree {os.path.join(RUNS_DIR, os.listdir(RUNS_DIR)[-1])}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the functionality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original workflow functionality was extended to process multiple input files. For this to work, every workflow run is outputed into a subdirectory of the current run. The output files of all workflows are concatenated using one last @python_app called aggregate_workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@python_app\n",
    "def aggregate_workflows(inputs=[], outputs=[]):\n",
    "    output_json = []\n",
    "    for file in inputs:\n",
    "        workflow_json = json.load(open(file))\n",
    "        output_json.append(workflow_json)\n",
    "    output_file = open(outputs[0], 'w')\n",
    "    output_file.write(json.dumps(output_json))\n",
    "    output_file.close()\n",
    "\n",
    "\n",
    "def run_aggregate_workflows(aggregate_json_futures:List[AppFuture], rundir):\n",
    "    inputs = [f.outputs[0] for f in aggregate_json_futures]\n",
    "    outputs = [\n",
    "        'aggregated_workflows.json'\n",
    "    ]\n",
    "    outputs = format_files(rundir, outputs)\n",
    "    aggregate_workflows_future = aggregate_workflows(inputs=inputs, outputs=outputs)\n",
    "    return aggregate_workflows_future"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final parallel workflow implementation is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_workflows(files:List[File], rundir:str) -> AppFuture:\n",
    "    futures = []\n",
    "    for idx, input_file in enumerate(files):\n",
    "        workflow_dir = generate_workflow_dir(rundir, idx)\n",
    "        workflow_future = run_workflow(input_file, workflow_dir)\n",
    "        futures.append(workflow_future)\n",
    "        sleep(0.5)\n",
    "\n",
    "    print(\"\\nScheduling Workflow Aggregation\")\n",
    "    last_future = run_aggregate_workflows(futures, rundir)\n",
    "    print(last_future)\n",
    "    return last_future"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, the test function launches 3 workflows over the same input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parallel_workflows():\n",
    "    rundir = generate_run_dir(RUNS_DIR)\n",
    "    input_file = os.path.join(DATA_DIR, 'VEP_raw.A25.mutect2.filtered.snp.vcf')\n",
    "    input_file = File(input_file)\n",
    "    files = []\n",
    "    for _ in range(3):\n",
    "        files.append(copy.deepcopy(input_file))\n",
    "\n",
    "    start = time()\n",
    "    last_future = run_parallel_workflows(files, rundir)\n",
    "    last_future.result()\n",
    "    end = time()\n",
    "    print(\"\\nAll Workflows Finished !\")\n",
    "    print(f\"Elapsed Time: {round(end - start, 2)} [seconds]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scheduling Workflow\n",
      "Input file: /home/alejo/Documents/NCSA/phyloflow_parsl/example_data/VEP_raw.A25.mutect2.filtered.snp.vcf\n",
      "Output dir: /home/alejo/Documents/NCSA/phyloflow_parsl/parsl/runs/2023-06-22_10:47:56/run_workflow_0\n",
      "<AppFuture at 0x7f77e6aa2450 state=pending>\n",
      "<AppFuture at 0x7f77e6aa3250 state=pending>\n",
      "<AppFuture at 0x7f77e6aa3810 state=pending>\n",
      "<AppFuture at 0x7f77e6aa43d0 state=pending>\n",
      "<AppFuture at 0x7f77e6aa5e50 state=pending>\n",
      "\n",
      "Scheduling Workflow\n",
      "Input file: /home/alejo/Documents/NCSA/phyloflow_parsl/example_data/VEP_raw.A25.mutect2.filtered.snp.vcf\n",
      "Output dir: /home/alejo/Documents/NCSA/phyloflow_parsl/parsl/runs/2023-06-22_10:47:56/run_workflow_1\n",
      "<AppFuture at 0x7f77e6aa5b50 state=pending>\n",
      "<AppFuture at 0x7f77e6aa7d50 state=pending>\n",
      "<AppFuture at 0x7f77e6ab47d0 state=pending>\n",
      "<AppFuture at 0x7f77e6ab5f90 state=pending>\n",
      "<AppFuture at 0x7f77e6b69c50 state=pending>\n",
      "\n",
      "Scheduling Workflow\n",
      "Input file: /home/alejo/Documents/NCSA/phyloflow_parsl/example_data/VEP_raw.A25.mutect2.filtered.snp.vcf\n",
      "Output dir: /home/alejo/Documents/NCSA/phyloflow_parsl/parsl/runs/2023-06-22_10:47:56/run_workflow_2\n",
      "<AppFuture at 0x7f77e6a8ccd0 state=pending>\n",
      "<AppFuture at 0x7f77e6b4d490 state=pending>\n",
      "<AppFuture at 0x7f77e6b45410 state=pending>\n",
      "<AppFuture at 0x7f77e6ab5310 state=pending>\n",
      "<AppFuture at 0x7f77e6ab7cd0 state=pending>\n",
      "\n",
      "Scheduling Workflow Aggregation\n",
      "<AppFuture at 0x7f77e6abcd50 state=pending>\n",
      "\n",
      "All Workflows Finished !\n",
      "Elapsed Time: 7.41 [seconds]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_parallel_workflows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker Container"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/docker.png\" height=\"100\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WDL is designed to run every single task in an independent container, so the original phyloflow had different docker images for every task. That is not the case for Parsl, however a workaround was to create multiple conda environments within the same docker container, so that parsl apps can execute the code within the corresponding environment.\n",
    "\n",
    "Attached to the project is the dockerfile ([/parsl/dockerfile](./parsl/dockerfile)) that contains the definition to build a docker image with all the dependencies that the workflow needs, including the conda environments. The file [/parsl/docker_commands.sh](./parsl/docker_commands.sh) contains docker commands to build the image and run the container. \n",
    "\n",
    "The container has been tested on Linux and Mac Systems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsl Pros"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parsl gives you finer control over the dependencies\n",
    "* Parsl provides an easy way to extend workflows using native python functionality\n",
    "* Parsl workflows are easy to parallelize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsl Cons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parsl requieres more knowledge of the filesystem\n",
    "* It is harder in Parsl to run tasks that have conflicting dependencies\n",
    "* Retrieving inputs and outputs by indexing an array can get confusing and error prone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phyloflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
